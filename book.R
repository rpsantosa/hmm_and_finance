#https://www.r-bloggers.com/fun-with-r-and-hmms/
temp <- tempfile()
link<-'https://archive.org/download/BrownCorpus/brown.zip'
download.file(link,temp)
u<-unzip(temp)
###############################
# library(readr)
# mystring <- read_file(u[1] )
################################
nfunc<-function(i,u){
  corpus<-readChar(u[i],nchar=100000)
  a <- unlist (strsplit (gsub ("[^a-z]", "_", tolower (corpus)), ""))
  return(a)
}
a<-unlist(sapply(1:4,u,FUN=nfunc))

letter.labels=c("_", letters)


library (HMM)

library (lattice)

prob <- function (x) {x /sum(x)}  # Makes it a probability (it sums to 1)

hmm <- initHMM (c("A", "B"), letter.labels, startProbs=prob (runif (2)),
                
                transProbs= t(apply(matrix(runif (4), 2),2,prob)),
                
                emissionProbs= apply(matrix(runif (54),ncol=2),1,prob))

#Then, we use the Baum Welch algorithm to update the probabilities based on repeated iteration over the corpus:
  
  system.time (a.bw <- baumWelch (hmm, a))
  system.time (a.vt <- viterbiTraining(hmm, a))
  
  
  #system.time (a.bw <- baumWelch (hmm, a, 5))
  #would iterate at most 5 times. The results wonâ€™t be nearly as good as going, say, 50 iterations but you should see one of the two classes having larger bumps at the vowels and space. The graph at the top of the article was generated by:
    
    xyplot (a.bw$hmm$emissionProbs[1,] ~ c(1:27), scales=list(x=list(at=1:27, labels=colnames (a.bw$hmm$emissionProbs))), type='h')
#  and the other state is:
    xyplot (a.bw$hmm$emissionProbs[2,] ~ c(1:27), scales=list(x=list(at=1:27, labels=colnames (a.bw$hmm$emissionProbs))), type='h')
    
    
    xyplot (a.vt$hmm$emissionProbs[1,] ~ c(1:27), scales=list(x=list(at=1:27, labels=colnames (a.vt$hmm$emissionProbs))), type='h')
    #  and the other state is:
    xyplot (a.vt$hmm$emissionProbs[2,] ~ c(1:27), scales=list(x=list(at=1:27, labels=colnames (a.vt$hmm$emissionProbs))), type='h')
  